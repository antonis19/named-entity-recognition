{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition using CRF model\n",
    "In Natural Language Processing (NLP) an Entity Recognition is one of the common problem. The entity is referred to as the part of the text that is interested in. In NLP, NER is a method of extracting the relevant information from a large corpus and classifying those entities into predefined categories such as location, organization, name and so on. \n",
    "Information about lables: \n",
    "* geo = Geographical Entity\n",
    "* org = Organization\n",
    "* per = Person\n",
    "* gpe = Geopolitical Entity\n",
    "* tim = Time indicator\n",
    "* art = Artifact\n",
    "* eve = Event\n",
    "* nat = Natural Phenomenon\n",
    "\n",
    "        1. Total Words Count = 1354149 \n",
    "        2. Target Data Column: Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_f1_score\n",
    "from sklearn_crfsuite.metrics import flat_classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the csv file\n",
    "df = pd.read_csv('./data/ner_dataset.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>British</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>troops</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #           Word  POS    Tag\n",
       "0   Sentence: 1      Thousands  NNS      O\n",
       "1           NaN             of   IN      O\n",
       "2           NaN  demonstrators  NNS      O\n",
       "3           NaN           have  VBP      O\n",
       "4           NaN        marched  VBN      O\n",
       "5           NaN        through   IN      O\n",
       "6           NaN         London  NNP  B-geo\n",
       "7           NaN             to   TO      O\n",
       "8           NaN        protest   VB      O\n",
       "9           NaN            the   DT      O\n",
       "10          NaN            war   NN      O\n",
       "11          NaN             in   IN      O\n",
       "12          NaN           Iraq  NNP  B-geo\n",
       "13          NaN            and   CC      O\n",
       "14          NaN         demand   VB      O\n",
       "15          NaN            the   DT      O\n",
       "16          NaN     withdrawal   NN      O\n",
       "17          NaN             of   IN      O\n",
       "18          NaN        British   JJ  B-gpe\n",
       "19          NaN         troops  NNS      O"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display first 10 rows\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>47959</td>\n",
       "      <td>1048575</td>\n",
       "      <td>1048575</td>\n",
       "      <td>1048575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>47959</td>\n",
       "      <td>35178</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Sentence: 4213</td>\n",
       "      <td>the</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>52573</td>\n",
       "      <td>145807</td>\n",
       "      <td>887908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Sentence #     Word      POS      Tag\n",
       "count            47959  1048575  1048575  1048575\n",
       "unique           47959    35178       42       17\n",
       "top     Sentence: 4213      the       NN        O\n",
       "freq                 1    52573   145807   887908"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations : \n",
    "* There are total 47959 sentences in the dataset.\n",
    "* Number unique words in the dataset are 35178.\n",
    "* Total 17 lables (Tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'B-geo', 'B-gpe', 'B-per', 'I-geo', 'B-org', 'I-org', 'B-tim',\n",
       "       'B-art', 'I-art', 'I-per', 'I-gpe', 'I-tim', 'B-nat', 'B-eve',\n",
       "       'I-eve', 'I-nat'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the unique Tags\n",
    "df['Tag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence #    1000616\n",
       "Word                0\n",
       "POS                 0\n",
       "Tag                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking null values, if any.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of missing values in 'Sentence #' attribute. So we will use pandas fillna technique and use 'ffill' method which propagates last valid observation forward to next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(method = 'ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS    Tag\n",
       "0  Sentence: 1      Thousands  NNS      O\n",
       "1  Sentence: 1             of   IN      O\n",
       "2  Sentence: 1  demonstrators  NNS      O\n",
       "3  Sentence: 1           have  VBP      O\n",
       "4  Sentence: 1        marched  VBN      O\n",
       "5  Sentence: 1        through   IN      O\n",
       "6  Sentence: 1         London  NNP  B-geo\n",
       "7  Sentence: 1             to   TO      O\n",
       "8  Sentence: 1        protest   VB      O\n",
       "9  Sentence: 1            the   DT      O"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a class te get sentence. The each sentence will be list of tuples with its tag and pos.\n",
    "class sentence(object):\n",
    "    def __init__(self, df):\n",
    "        self.n_sent = 1\n",
    "        self.df = df\n",
    "        self.empty = False\n",
    "        agg = lambda s : [(w, p, t) for w, p, t in zip(s['Word'].values.tolist(),\n",
    "                                                       s['POS'].values.tolist(),\n",
    "                                                       s['Tag'].values.tolist())]\n",
    "        self.grouped = self.df.groupby(\"Sentence #\").apply(agg)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    def get_text(self):\n",
    "        try:\n",
    "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
    "            self.n_sent +=1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying one full sentence\n",
    "getter = sentence(df)\n",
    "sentences = [\" \".join([s[0] for s in sent]) for sent in getter.sentences]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47959"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Thousands', 'NNS', 'O'), ('of', 'IN', 'O'), ('demonstrators', 'NNS', 'O'), ('have', 'VBP', 'O'), ('marched', 'VBN', 'O'), ('through', 'IN', 'O'), ('London', 'NNP', 'B-geo'), ('to', 'TO', 'O'), ('protest', 'VB', 'O'), ('the', 'DT', 'O'), ('war', 'NN', 'O'), ('in', 'IN', 'O'), ('Iraq', 'NNP', 'B-geo'), ('and', 'CC', 'O'), ('demand', 'VB', 'O'), ('the', 'DT', 'O'), ('withdrawal', 'NN', 'O'), ('of', 'IN', 'O'), ('British', 'JJ', 'B-gpe'), ('troops', 'NNS', 'O'), ('from', 'IN', 'O'), ('that', 'DT', 'O'), ('country', 'NN', 'O'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "#sentence with its pos and tag.\n",
    "sent = getter.get_text()\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all the sentences in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thousands', 'NNS', 'O'),\n",
       " ('of', 'IN', 'O'),\n",
       " ('demonstrators', 'NNS', 'O'),\n",
       " ('have', 'VBP', 'O'),\n",
       " ('marched', 'VBN', 'O'),\n",
       " ('through', 'IN', 'O'),\n",
       " ('London', 'NNP', 'B-geo'),\n",
       " ('to', 'TO', 'O'),\n",
       " ('protest', 'VB', 'O'),\n",
       " ('the', 'DT', 'O'),\n",
       " ('war', 'NN', 'O'),\n",
       " ('in', 'IN', 'O'),\n",
       " ('Iraq', 'NNP', 'B-geo'),\n",
       " ('and', 'CC', 'O'),\n",
       " ('demand', 'VB', 'O'),\n",
       " ('the', 'DT', 'O'),\n",
       " ('withdrawal', 'NN', 'O'),\n",
       " ('of', 'IN', 'O'),\n",
       " ('British', 'JJ', 'B-gpe'),\n",
       " ('troops', 'NNS', 'O'),\n",
       " ('from', 'IN', 'O'),\n",
       " ('that', 'DT', 'O'),\n",
       " ('country', 'NN', 'O'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Preparation\n",
    "These are the default features used by the NER in nltk. We can also modify it for our customization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [sent2features(s) for s in sentences]\n",
    "y = [sent2labels(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thousands'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bias': 1.0,\n",
       "  'word.lower()': 'thousands',\n",
       "  'word[-3:]': 'nds',\n",
       "  'word[-2:]': 'ds',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NNS',\n",
       "  'postag[:2]': 'NN',\n",
       "  'BOS': True,\n",
       "  '+1:word.lower()': 'of',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'IN',\n",
       "  '+1:postag[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'of',\n",
       "  'word[-3:]': 'of',\n",
       "  'word[-2:]': 'of',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'IN',\n",
       "  'postag[:2]': 'IN',\n",
       "  '-1:word.lower()': 'thousands',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'NNS',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  '+1:word.lower()': 'demonstrators',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'NNS',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'demonstrators',\n",
       "  'word[-3:]': 'ors',\n",
       "  'word[-2:]': 'rs',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NNS',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': 'of',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'IN',\n",
       "  '-1:postag[:2]': 'IN',\n",
       "  '+1:word.lower()': 'have',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'VBP',\n",
       "  '+1:postag[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'have',\n",
       "  'word[-3:]': 'ave',\n",
       "  'word[-2:]': 've',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'VBP',\n",
       "  'postag[:2]': 'VB',\n",
       "  '-1:word.lower()': 'demonstrators',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'NNS',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  '+1:word.lower()': 'marched',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'VBN',\n",
       "  '+1:postag[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'marched',\n",
       "  'word[-3:]': 'hed',\n",
       "  'word[-2:]': 'ed',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'VBN',\n",
       "  'postag[:2]': 'VB',\n",
       "  '-1:word.lower()': 'have',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'VBP',\n",
       "  '-1:postag[:2]': 'VB',\n",
       "  '+1:word.lower()': 'through',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'IN',\n",
       "  '+1:postag[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'through',\n",
       "  'word[-3:]': 'ugh',\n",
       "  'word[-2:]': 'gh',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'IN',\n",
       "  'postag[:2]': 'IN',\n",
       "  '-1:word.lower()': 'marched',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'VBN',\n",
       "  '-1:postag[:2]': 'VB',\n",
       "  '+1:word.lower()': 'london',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'NNP',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'london',\n",
       "  'word[-3:]': 'don',\n",
       "  'word[-2:]': 'on',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NNP',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': 'through',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'IN',\n",
       "  '-1:postag[:2]': 'IN',\n",
       "  '+1:word.lower()': 'to',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'TO',\n",
       "  '+1:postag[:2]': 'TO'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'to',\n",
       "  'word[-3:]': 'to',\n",
       "  'word[-2:]': 'to',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'TO',\n",
       "  'postag[:2]': 'TO',\n",
       "  '-1:word.lower()': 'london',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'NNP',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  '+1:word.lower()': 'protest',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'VB',\n",
       "  '+1:postag[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'protest',\n",
       "  'word[-3:]': 'est',\n",
       "  'word[-2:]': 'st',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'VB',\n",
       "  'postag[:2]': 'VB',\n",
       "  '-1:word.lower()': 'to',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'TO',\n",
       "  '-1:postag[:2]': 'TO',\n",
       "  '+1:word.lower()': 'the',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'DT',\n",
       "  '+1:postag[:2]': 'DT'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'the',\n",
       "  'word[-3:]': 'the',\n",
       "  'word[-2:]': 'he',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'DT',\n",
       "  'postag[:2]': 'DT',\n",
       "  '-1:word.lower()': 'protest',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'VB',\n",
       "  '-1:postag[:2]': 'VB',\n",
       "  '+1:word.lower()': 'war',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'NN',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'war',\n",
       "  'word[-3:]': 'war',\n",
       "  'word[-2:]': 'ar',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NN',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': 'the',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'DT',\n",
       "  '-1:postag[:2]': 'DT',\n",
       "  '+1:word.lower()': 'in',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'IN',\n",
       "  '+1:postag[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'in',\n",
       "  'word[-3:]': 'in',\n",
       "  'word[-2:]': 'in',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'IN',\n",
       "  'postag[:2]': 'IN',\n",
       "  '-1:word.lower()': 'war',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'NN',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  '+1:word.lower()': 'iraq',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'NNP',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'iraq',\n",
       "  'word[-3:]': 'raq',\n",
       "  'word[-2:]': 'aq',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NNP',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': 'in',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'IN',\n",
       "  '-1:postag[:2]': 'IN',\n",
       "  '+1:word.lower()': 'and',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'CC',\n",
       "  '+1:postag[:2]': 'CC'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'and',\n",
       "  'word[-3:]': 'and',\n",
       "  'word[-2:]': 'nd',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'CC',\n",
       "  'postag[:2]': 'CC',\n",
       "  '-1:word.lower()': 'iraq',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'NNP',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  '+1:word.lower()': 'demand',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'VB',\n",
       "  '+1:postag[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'demand',\n",
       "  'word[-3:]': 'and',\n",
       "  'word[-2:]': 'nd',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'VB',\n",
       "  'postag[:2]': 'VB',\n",
       "  '-1:word.lower()': 'and',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'CC',\n",
       "  '-1:postag[:2]': 'CC',\n",
       "  '+1:word.lower()': 'the',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'DT',\n",
       "  '+1:postag[:2]': 'DT'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'the',\n",
       "  'word[-3:]': 'the',\n",
       "  'word[-2:]': 'he',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'DT',\n",
       "  'postag[:2]': 'DT',\n",
       "  '-1:word.lower()': 'demand',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'VB',\n",
       "  '-1:postag[:2]': 'VB',\n",
       "  '+1:word.lower()': 'withdrawal',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'NN',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'withdrawal',\n",
       "  'word[-3:]': 'wal',\n",
       "  'word[-2:]': 'al',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NN',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': 'the',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'DT',\n",
       "  '-1:postag[:2]': 'DT',\n",
       "  '+1:word.lower()': 'of',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'IN',\n",
       "  '+1:postag[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'of',\n",
       "  'word[-3:]': 'of',\n",
       "  'word[-2:]': 'of',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'IN',\n",
       "  'postag[:2]': 'IN',\n",
       "  '-1:word.lower()': 'withdrawal',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'NN',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  '+1:word.lower()': 'british',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'JJ',\n",
       "  '+1:postag[:2]': 'JJ'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'british',\n",
       "  'word[-3:]': 'ish',\n",
       "  'word[-2:]': 'sh',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'JJ',\n",
       "  'postag[:2]': 'JJ',\n",
       "  '-1:word.lower()': 'of',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'IN',\n",
       "  '-1:postag[:2]': 'IN',\n",
       "  '+1:word.lower()': 'troops',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'NNS',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'troops',\n",
       "  'word[-3:]': 'ops',\n",
       "  'word[-2:]': 'ps',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NNS',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': 'british',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'JJ',\n",
       "  '-1:postag[:2]': 'JJ',\n",
       "  '+1:word.lower()': 'from',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'IN',\n",
       "  '+1:postag[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'from',\n",
       "  'word[-3:]': 'rom',\n",
       "  'word[-2:]': 'om',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'IN',\n",
       "  'postag[:2]': 'IN',\n",
       "  '-1:word.lower()': 'troops',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'NNS',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  '+1:word.lower()': 'that',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'DT',\n",
       "  '+1:postag[:2]': 'DT'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'that',\n",
       "  'word[-3:]': 'hat',\n",
       "  'word[-2:]': 'at',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'DT',\n",
       "  'postag[:2]': 'DT',\n",
       "  '-1:word.lower()': 'from',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'IN',\n",
       "  '-1:postag[:2]': 'IN',\n",
       "  '+1:word.lower()': 'country',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'NN',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'country',\n",
       "  'word[-3:]': 'try',\n",
       "  'word[-2:]': 'ry',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NN',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': 'that',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'DT',\n",
       "  '-1:postag[:2]': 'DT',\n",
       "  '+1:word.lower()': '.',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': '.',\n",
       "  '+1:postag[:2]': '.'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '.',\n",
       "  'word[-3:]': '.',\n",
       "  'word[-2:]': '.',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': '.',\n",
       "  'postag[:2]': '.',\n",
       "  '-1:word.lower()': 'country',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'NN',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  'EOS': True}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|██████████| 38367/38367 [00:10<00:00, 3490.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 0\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 138168\n",
      "Seconds required: 2.709\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.100000\n",
      "c2: 0.100000\n",
      "num_memories: 6\n",
      "max_iterations: 100\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=2.23  loss=1349607.11 active=137152 feature_norm=1.00\n",
      "Iter 2   time=2.17  loss=1057747.56 active=135739 feature_norm=4.40\n",
      "Iter 3   time=1.11  loss=826684.80 active=130052 feature_norm=3.85\n",
      "Iter 4   time=5.29  loss=453862.58 active=131590 feature_norm=3.25\n",
      "Iter 5   time=1.14  loss=380242.30 active=133381 feature_norm=4.09\n",
      "Iter 6   time=1.21  loss=294798.54 active=131936 feature_norm=5.88\n",
      "Iter 7   time=1.06  loss=255885.17 active=124832 feature_norm=7.22\n",
      "Iter 8   time=1.09  loss=227569.97 active=118801 feature_norm=8.23\n",
      "Iter 9   time=1.05  loss=196801.27 active=110599 feature_norm=10.45\n",
      "Iter 10  time=1.14  loss=177968.58 active=107739 feature_norm=11.72\n",
      "Iter 11  time=1.13  loss=164854.67 active=105319 feature_norm=13.11\n",
      "Iter 12  time=1.08  loss=154481.94 active=102237 feature_norm=14.54\n",
      "Iter 13  time=1.06  loss=146853.27 active=99517 feature_norm=15.91\n",
      "Iter 14  time=1.22  loss=137241.41 active=98964 feature_norm=16.35\n",
      "Iter 15  time=1.13  loss=128010.35 active=96818 feature_norm=17.85\n",
      "Iter 16  time=1.10  loss=121080.48 active=95861 feature_norm=19.21\n",
      "Iter 17  time=1.08  loss=115033.59 active=95117 feature_norm=20.99\n",
      "Iter 18  time=1.08  loss=110380.34 active=94733 feature_norm=22.30\n",
      "Iter 19  time=1.10  loss=104946.22 active=93439 feature_norm=24.14\n",
      "Iter 20  time=1.22  loss=98127.31 active=91673 feature_norm=26.68\n",
      "Iter 21  time=1.26  loss=92325.42 active=90293 feature_norm=30.97\n",
      "Iter 22  time=1.26  loss=87228.88 active=90305 feature_norm=33.16\n",
      "Iter 23  time=1.31  loss=82910.40 active=89504 feature_norm=36.69\n",
      "Iter 24  time=1.18  loss=80384.70 active=87028 feature_norm=43.67\n",
      "Iter 25  time=1.14  loss=74691.41 active=87261 feature_norm=46.18\n",
      "Iter 26  time=1.16  loss=72745.39 active=87209 feature_norm=48.06\n",
      "Iter 27  time=1.13  loss=68446.68 active=85102 feature_norm=57.30\n",
      "Iter 28  time=1.21  loss=66976.74 active=85040 feature_norm=60.27\n",
      "Iter 29  time=1.25  loss=65231.90 active=85811 feature_norm=61.38\n",
      "Iter 30  time=1.27  loss=63805.56 active=85057 feature_norm=64.04\n",
      "Iter 31  time=1.22  loss=61732.48 active=84364 feature_norm=68.09\n",
      "Iter 32  time=1.25  loss=59545.21 active=83519 feature_norm=74.30\n",
      "Iter 33  time=1.26  loss=57486.33 active=83329 feature_norm=79.24\n",
      "Iter 34  time=1.18  loss=55877.19 active=82964 feature_norm=83.38\n",
      "Iter 35  time=1.23  loss=53558.41 active=81514 feature_norm=92.62\n",
      "Iter 36  time=1.18  loss=51989.86 active=81599 feature_norm=96.81\n",
      "Iter 37  time=1.20  loss=50353.15 active=81089 feature_norm=103.20\n",
      "Iter 38  time=1.17  loss=48365.39 active=79421 feature_norm=113.06\n",
      "Iter 39  time=1.19  loss=46987.64 active=78697 feature_norm=122.45\n",
      "Iter 40  time=1.17  loss=45692.55 active=78702 feature_norm=127.09\n",
      "Iter 41  time=1.10  loss=44278.45 active=77214 feature_norm=136.43\n",
      "Iter 42  time=1.22  loss=42856.49 active=76298 feature_norm=147.12\n",
      "Iter 43  time=1.13  loss=41748.60 active=75318 feature_norm=158.54\n",
      "Iter 44  time=1.15  loss=40797.27 active=74746 feature_norm=167.51\n",
      "Iter 45  time=1.24  loss=40110.37 active=74176 feature_norm=174.20\n",
      "Iter 46  time=1.17  loss=39865.38 active=72756 feature_norm=187.54\n",
      "Iter 47  time=1.16  loss=39039.33 active=73181 feature_norm=189.96\n",
      "Iter 48  time=1.14  loss=38865.95 active=73228 feature_norm=191.67\n",
      "Iter 49  time=1.24  loss=38379.15 active=71839 feature_norm=200.41\n",
      "Iter 50  time=1.19  loss=38152.71 active=71639 feature_norm=202.27\n",
      "Iter 51  time=1.14  loss=37896.15 active=71128 feature_norm=203.83\n",
      "Iter 52  time=1.23  loss=37665.90 active=70821 feature_norm=205.76\n",
      "Iter 53  time=1.19  loss=37465.24 active=70381 feature_norm=207.81\n",
      "Iter 54  time=1.17  loss=37304.69 active=70202 feature_norm=207.92\n",
      "Iter 55  time=1.06  loss=37188.87 active=70021 feature_norm=208.78\n",
      "Iter 56  time=1.19  loss=36986.80 active=69354 feature_norm=209.99\n",
      "Iter 57  time=1.07  loss=36825.22 active=68963 feature_norm=210.76\n",
      "Iter 58  time=1.19  loss=36681.83 active=68640 feature_norm=211.35\n",
      "Iter 59  time=1.08  loss=36529.43 active=67466 feature_norm=212.49\n",
      "Iter 60  time=1.06  loss=36386.39 active=66692 feature_norm=213.14\n",
      "Iter 61  time=1.16  loss=36263.87 active=66205 feature_norm=214.01\n",
      "Iter 62  time=1.30  loss=36148.37 active=65910 feature_norm=214.78\n",
      "Iter 63  time=1.16  loss=36045.72 active=65714 feature_norm=215.65\n",
      "Iter 64  time=1.23  loss=35953.41 active=65554 feature_norm=216.23\n",
      "Iter 65  time=1.28  loss=35877.19 active=65218 feature_norm=217.06\n",
      "Iter 66  time=1.18  loss=35801.15 active=65105 feature_norm=217.57\n",
      "Iter 67  time=1.17  loss=35729.48 active=64840 feature_norm=218.19\n",
      "Iter 68  time=1.18  loss=35655.53 active=64434 feature_norm=218.65\n",
      "Iter 69  time=1.07  loss=35593.88 active=64149 feature_norm=219.23\n",
      "Iter 70  time=1.06  loss=35536.62 active=63991 feature_norm=219.55\n",
      "Iter 71  time=1.14  loss=35484.50 active=63808 feature_norm=220.01\n",
      "Iter 72  time=1.10  loss=35436.85 active=63661 feature_norm=220.23\n",
      "Iter 73  time=1.08  loss=35398.36 active=63273 feature_norm=220.59\n",
      "Iter 74  time=1.09  loss=35355.80 active=63142 feature_norm=220.75\n",
      "Iter 75  time=1.08  loss=35319.20 active=62973 feature_norm=220.97\n",
      "Iter 76  time=1.30  loss=35282.90 active=62758 feature_norm=221.07\n",
      "Iter 77  time=1.06  loss=35251.04 active=62571 feature_norm=221.29\n",
      "Iter 78  time=1.05  loss=35218.27 active=62499 feature_norm=221.37\n",
      "Iter 79  time=1.16  loss=35187.69 active=62339 feature_norm=221.61\n",
      "Iter 80  time=1.16  loss=35160.92 active=62207 feature_norm=221.69\n",
      "Iter 81  time=1.08  loss=35135.00 active=62136 feature_norm=221.91\n",
      "Iter 82  time=1.26  loss=35110.98 active=62039 feature_norm=222.00\n",
      "Iter 83  time=1.17  loss=35088.98 active=61903 feature_norm=222.18\n",
      "Iter 84  time=1.24  loss=35068.71 active=61784 feature_norm=222.28\n",
      "Iter 85  time=1.12  loss=35047.76 active=61674 feature_norm=222.49\n",
      "Iter 86  time=1.10  loss=35027.36 active=61631 feature_norm=222.59\n",
      "Iter 87  time=1.15  loss=35008.32 active=61560 feature_norm=222.77\n",
      "Iter 88  time=1.10  loss=34990.61 active=61506 feature_norm=222.87\n",
      "Iter 89  time=1.14  loss=34972.89 active=61432 feature_norm=223.07\n",
      "Iter 90  time=1.29  loss=34957.73 active=61384 feature_norm=223.18\n",
      "Iter 91  time=1.15  loss=34939.36 active=61346 feature_norm=223.37\n",
      "Iter 92  time=1.12  loss=34925.71 active=61308 feature_norm=223.48\n",
      "Iter 93  time=1.18  loss=34908.37 active=61240 feature_norm=223.66\n",
      "Iter 94  time=1.16  loss=34896.14 active=61168 feature_norm=223.78\n",
      "Iter 95  time=1.22  loss=34879.84 active=61121 feature_norm=223.96\n",
      "Iter 96  time=1.24  loss=34868.46 active=61089 feature_norm=224.06\n",
      "Iter 97  time=1.20  loss=34852.75 active=61051 feature_norm=224.23\n",
      "Iter 98  time=1.17  loss=34842.47 active=61004 feature_norm=224.34\n",
      "Iter 99  time=1.09  loss=34827.69 active=60962 feature_norm=224.50\n",
      "Iter 100 time=1.09  loss=34818.02 active=60922 feature_norm=224.61\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 122.288\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 60922 (138168)\n",
      "Number of active attributes: 30415 (93062)\n",
      "Number of active labels: 17 (17)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.138\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=False, c1=0.1, c2=0.1,\n",
       "    keep_tempfiles=None, max_iterations=100, verbose=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = CRF(algorithm = 'lbfgs',\n",
    "         c1 = 0.1,\n",
    "         c2 = 0.1,\n",
    "         max_iterations = 100,\n",
    "         all_possible_transitions = False, verbose = True)\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting on the test set.\n",
    "y_pred = crf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model performance.\n",
    "We will use precision, recall and f1-score metrics to evaluate the performance of the model since the accuracy is not a good metric for this dataset because we have an unequal number of data points in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9718998419615057\n"
     ]
    }
   ],
   "source": [
    "f1_score = flat_f1_score(y_test, y_pred, average = 'weighted')\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wc72da/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:71: FutureWarning: Pass labels=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.55      0.19      0.28        90\n",
      "       B-eve       0.57      0.40      0.47        65\n",
      "       B-geo       0.87      0.91      0.89      7534\n",
      "       B-gpe       0.97      0.95      0.96      3188\n",
      "       B-nat       0.81      0.36      0.50        47\n",
      "       B-org       0.81      0.74      0.77      4033\n",
      "       B-per       0.84      0.84      0.84      3325\n",
      "       B-tim       0.93      0.88      0.90      4088\n",
      "       I-art       0.17      0.04      0.06        56\n",
      "       I-eve       0.38      0.25      0.31        51\n",
      "       I-geo       0.81      0.80      0.81      1477\n",
      "       I-gpe       0.96      0.51      0.67        43\n",
      "       I-nat       0.83      0.71      0.77         7\n",
      "       I-org       0.83      0.79      0.81      3366\n",
      "       I-per       0.84      0.90      0.87      3400\n",
      "       I-tim       0.85      0.77      0.81      1326\n",
      "           O       0.99      0.99      0.99    176846\n",
      "\n",
      "    accuracy                           0.97    208942\n",
      "   macro avg       0.77      0.65      0.69    208942\n",
      "weighted avg       0.97      0.97      0.97    208942\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = flat_classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks quite nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'I-tim',\n",
       " 'O']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
